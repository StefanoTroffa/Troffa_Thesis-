{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8792ee0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a92fa4",
   "metadata": {},
   "source": [
    "Here we initialize the parameters for showing \"better\" plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3084f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage.util import montage\n",
    "from IPython.display import Image, display, SVG, clear_output, HTML\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 6)\n",
    "plt.rcParams[\"figure.dpi\"] = 125\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.rcParams['font.family'] = ['sans-serif']\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style(\"whitegrid\", {'axes.grid': False})\n",
    "plt.rcParams['image.cmap'] = 'gray' # grayscale looks better\n",
    "import networkx as nx\n",
    "def draw_graph_mpl(g, pos=None, ax=None, layout_func=nx.drawing.layout.kamada_kawai_layout, draw_labels=True):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(20, 20))\n",
    "    else:\n",
    "        fig = None\n",
    "    if pos is None:\n",
    "        pos = layout_func(g)\n",
    "    node_color = []\n",
    "    node_labels = {}\n",
    "    shift_pos = {}\n",
    "    for k in g:\n",
    "        node_color.append(g.nodes[k].get('color', 'green'))\n",
    "        node_labels[k] = g.nodes[k].get('label', k)\n",
    "        shift_pos[k] = [pos[k][0], pos[k][1]]\n",
    "    \n",
    "    edge_color = []\n",
    "    edge_width = []\n",
    "    for e in g.edges():\n",
    "        edge_color.append(g.edges[e].get('color', 'black'))\n",
    "        edge_width.append(g.edges[e].get('width', 0.5))\n",
    "    nx.draw_networkx_edges(g, pos, style='--', edge_color=edge_color, width=edge_width, alpha=0.5, ax=ax)\n",
    "    nx.draw_networkx_nodes(g, pos, node_color=node_color, node_shape='p', node_size=300, alpha=0.75, ax=ax)\n",
    "    if draw_labels:\n",
    "        nx.draw_networkx_labels(g, shift_pos, labels=node_labels, ax=ax)\n",
    "    ax.autoscale()\n",
    "    return fig, ax, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7745f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import Input, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from spektral.datasets import mnist\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "from spektral.utils import normalized_laplacian\n",
    "from spektral.data import MixedLoader, BatchLoader\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import sparse_categorical_accuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from spektral.data import MixedLoader\n",
    "from spektral.datasets.mnist import MNIST\n",
    "from spektral.layers import GATConv, GCNConv, GeneralConv, GlobalSumPool\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "\n",
    "# Parameters\n",
    "batch_size = 50  # Batch size\n",
    "epochs = 1000  # Number of training epochs\n",
    "patience = 10  # Patience for early stopping\n",
    "l2_reg = 5e-4  # Regularization rate for l2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b7d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MNIST()\n",
    "adj=data.a\n",
    "\n",
    "# Create filter for GCN and convert to sparse tensor.\n",
    "data.a = GeneralConv.preprocess(data.a)\n",
    "data.a = sp_matrix_to_sp_tensor(data.a)\n",
    "\n",
    "# Train/valid/test split\n",
    "data_tr, data_te = data[:-10000], data[-10000:]\n",
    "data_tr, data_va = data_tr[:-10000], data_tr[-10000:]\n",
    "\n",
    "# We use a MixedLoader since the dataset is in mixed mode\n",
    "loader_tr = MixedLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = MixedLoader(data_va, batch_size=batch_size)\n",
    "loader_te = MixedLoader(data_te, batch_size=batch_size)\n",
    "\n",
    "# Specify device to be used by TensorFlow\n",
    "device = '/device:GPU:0'\n",
    "\n",
    "# Build model\n",
    "class Net(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv1 = GeneralConv(20, activation=\"elu\", kernel_regularizer=l2(l2_reg), use_bias=True)\n",
    "        self.conv2 = GeneralConv(20, activation=\"elu\", kernel_regularizer=l2(l2_reg), use_bias=True)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = Dense(256, activation=\"relu\")\n",
    "        self.fc2 = Dense(10, activation=\"softmax\")  # MNIST has 10 classes\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        with tf.device(device):\n",
    "            x = self.conv1([x, a])\n",
    "            x = self.conv2([x, a])\n",
    "            output = self.flatten(x)\n",
    "            output = self.fc1(output)\n",
    "            output = self.fc2(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Create model\n",
    "with tf.device(device):\n",
    "    model = Net()\n",
    "    optimizer = Adam()\n",
    "    loss_fn = SparseCategoricalCrossentropy()\n",
    "    fltr = normalized_laplacian(adj)\n",
    "\n",
    "# Training function\n",
    "@tf.function\n",
    "def train_on_batch(inputs, target):\n",
    "    with tf.device(device):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(inputs, training=True)\n",
    "            loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "            acc = tf.reduce_mean(sparse_categorical_accuracy(target, predictions))\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss, acc\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(loader):\n",
    "    step = 0\n",
    "    results = []\n",
    "    for batch in loader:\n",
    "        step += 1\n",
    "        inputs,target = batch\n",
    "        \n",
    "        with tf.device(device):\n",
    "            predictions = model(inputs, training=False)\n",
    "            loss = loss_fn(target, predictions)\n",
    "            acc = tf.reduce_mean(sparse_categorical_accuracy(target, predictions))\n",
    "            results.append((loss, acc, len(target)))  # Keep track of batch size\n",
    "            if step == loader.steps_per_epoch:\n",
    "                results = np.array(results)\n",
    "                return np.average(results[:, :-1], 0, weights=results[:, -1])\n",
    "            \n",
    "\n",
    "\n",
    "# Setup training\n",
    "best_val_loss = 10000\n",
    "current_patience = patience\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9a036",
   "metadata": {},
   "source": [
    "We note that there is a strong bottleneck in training time given by the Dataset structure that is slow to open, each epoch therefore can not take less time then what is needed to loop over the loader_tr dataset for one epoch: aka 17sec on this device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a034000a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 17.80702519416809\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time=time.time()\n",
    "step=0\n",
    "for batch in loader_tr:\n",
    "    step += 1\n",
    "    if step==50000:\n",
    "        break\n",
    "print(step, time.time()-start_time)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3e8503",
   "metadata": {},
   "source": [
    "By comparison, if we had to access only x_train, that are the numpy arrays containing the values of each pixel the time elapsed is close to 0. Thus the datastructure is clearly not efficient. I am going to check for similar structures on pytorch how the efficiency performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54d452e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 0.07930469512939453\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time=time.time()\n",
    "step=0\n",
    "for x in data_tr:\n",
    "    step += 1\n",
    "    a=x.x\n",
    "    b=x.y\n",
    "    c=x.a\n",
    "    \n",
    "    if step==50000:\n",
    "        break\n",
    "print(step, time.time()-start_time)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dbfe369",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.3263, acc: 0.8488 | Valid loss: 0.3932, acc: 0.8930 | Test loss: 0.3919, acc: 0.8908 | Elapsed time: 16.45s\n",
      "Train loss: 0.3316, acc: 0.9091 | Valid loss: 0.3551, acc: 0.9019 | Test loss: 0.3388, acc: 0.9008 | Elapsed time: 29.60s\n",
      "Train loss: 0.2968, acc: 0.9190 | Valid loss: 0.4556, acc: 0.8612 | Test loss: 0.3388, acc: 0.9008 | Elapsed time: 40.28s\n",
      "Train loss: 0.2502, acc: 0.9279 | Valid loss: 0.6306, acc: 0.8419 | Test loss: 0.3388, acc: 0.9008 | Elapsed time: 51.08s\n",
      "Train loss: 0.2449, acc: 0.9317 | Valid loss: 0.3061, acc: 0.9130 | Test loss: 0.3145, acc: 0.9134 | Elapsed time: 64.42s\n",
      "Train loss: 0.2386, acc: 0.9324 | Valid loss: 1.1207, acc: 0.7823 | Test loss: 0.3145, acc: 0.9134 | Elapsed time: 75.31s\n",
      "Train loss: 0.2254, acc: 0.9376 | Valid loss: 0.2274, acc: 0.9296 | Test loss: 0.2299, acc: 0.9318 | Elapsed time: 88.69s\n",
      "Train loss: 0.2129, acc: 0.9414 | Valid loss: 0.2729, acc: 0.9224 | Test loss: 0.2299, acc: 0.9318 | Elapsed time: 99.36s\n",
      "Train loss: 0.2097, acc: 0.9418 | Valid loss: 0.4970, acc: 0.8568 | Test loss: 0.2299, acc: 0.9318 | Elapsed time: 110.06s\n",
      "Train loss: 0.2032, acc: 0.9422 | Valid loss: 0.7305, acc: 0.7812 | Test loss: 0.2299, acc: 0.9318 | Elapsed time: 120.69s\n",
      "Train loss: 0.1968, acc: 0.9432 | Valid loss: 0.7459, acc: 0.7673 | Test loss: 0.2299, acc: 0.9318 | Elapsed time: 131.31s\n",
      "Train loss: 0.1869, acc: 0.9472 | Valid loss: 0.7710, acc: 0.8355 | Test loss: 0.2299, acc: 0.9318 | Elapsed time: 142.00s\n",
      "Train loss: 0.1786, acc: 0.9491 | Valid loss: 2.7541, acc: 0.5901 | Test loss: 0.2299, acc: 0.9318 | Elapsed time: 152.74s\n",
      "Train loss: 0.1726, acc: 0.9498 | Valid loss: 0.7853, acc: 0.8330 | Test loss: 0.2299, acc: 0.9318 | Elapsed time: 163.42s\n",
      "Train loss: 0.1669, acc: 0.9521 | Valid loss: 0.1626, acc: 0.9539 | Test loss: 0.1739, acc: 0.9514 | Elapsed time: 176.69s\n",
      "Train loss: 0.1644, acc: 0.9529 | Valid loss: 0.5747, acc: 0.8581 | Test loss: 0.1739, acc: 0.9514 | Elapsed time: 187.41s\n",
      "Train loss: 0.1602, acc: 0.9535 | Valid loss: 1.3679, acc: 0.6620 | Test loss: 0.1739, acc: 0.9514 | Elapsed time: 198.16s\n",
      "Train loss: 0.1539, acc: 0.9553 | Valid loss: 9.1306, acc: 0.6012 | Test loss: 0.1739, acc: 0.9514 | Elapsed time: 209.01s\n",
      "Train loss: 0.1519, acc: 0.9559 | Valid loss: 0.4844, acc: 0.8667 | Test loss: 0.1739, acc: 0.9514 | Elapsed time: 219.73s\n",
      "Train loss: 0.1418, acc: 0.9589 | Valid loss: 0.4380, acc: 0.9017 | Test loss: 0.1739, acc: 0.9514 | Elapsed time: 230.42s\n",
      "Train loss: 0.1469, acc: 0.9577 | Valid loss: 0.5898, acc: 0.8416 | Test loss: 0.1739, acc: 0.9514 | Elapsed time: 241.11s\n",
      "Train loss: 0.1371, acc: 0.9603 | Valid loss: 0.5388, acc: 0.8367 | Test loss: 0.1739, acc: 0.9514 | Elapsed time: 251.76s\n",
      "Train loss: 0.1328, acc: 0.9606 | Valid loss: 0.3856, acc: 0.9118 | Test loss: 0.1739, acc: 0.9514 | Elapsed time: 262.43s\n",
      "Train loss: 0.1292, acc: 0.9604 | Valid loss: 0.7769, acc: 0.8370 | Test loss: 0.1739, acc: 0.9514 | Elapsed time: 273.16s\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "results_tr = []\n",
    "step=0\n",
    "import time\n",
    "start_time=time.time()\n",
    "for batch in loader_tr:\n",
    "    step += 1\n",
    "  \n",
    "    # Training step\n",
    "    inputs, target = batch\n",
    "\n",
    "    loss, acc = train_on_batch(inputs, target)\n",
    "    results_tr.append((loss, acc, len(target)))\n",
    "    \n",
    "    if step == loader_tr.steps_per_epoch:\n",
    "        results_va = evaluate(loader_va)\n",
    "        if results_va[0] < best_val_loss:\n",
    "            best_val_loss = results_va[0]\n",
    "            current_patience = patience\n",
    "            results_te = evaluate(loader_te)\n",
    "        else:\n",
    "            current_patience -= 1\n",
    "            if current_patience == 0:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        # Print results\n",
    "        results_tr = np.array(results_tr)\n",
    "        results_tr = np.average(results_tr[:, :-1], 0, weights=results_tr[:, -1])\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        print(\n",
    "            \"Train loss: {:.4f}, acc: {:.4f} | \"\n",
    "            \"Valid loss: {:.4f}, acc: {:.4f} | \"\n",
    "            \"Test loss: {:.4f}, acc: {:.4f} | \"\n",
    "            \"Elapsed time: {:.2f}s\".format(\n",
    "                *results_tr, *results_va, *results_te, elapsed_time\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Reset epoch\n",
    "        results_tr = []\n",
    "        step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7daaf9c",
   "metadata": {},
   "source": [
    "Comments: It is unclear to me why the accuracy of the validation oscillates so wildly while the accuracy of test increases steadily. I would expect them to be correlated, and in general I don't understand why the validation loss can drop so low when the GNN is labelling so precisely the other dataset values.\n",
    "Having said that it seems that test accuracy is quite high, or at least acceptable I would say. This comes as no surprise, as the flatten layer brings 4mln parameters roughly (compared to 12 milions of the notebook that ispired this layer structure, it is still a bit better). Still need a fix the model.summary, because it is not possible to see the output shape of each layer. This is due to the way \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab7e5fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"net\"\n",
      "____________________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   Trainable  \n",
      "============================================================================\n",
      " general_conv (GeneralConv)  multiple                  120       Y          \n",
      "                                                                            \n",
      " general_conv_1 (GeneralConv  multiple                 500       Y          \n",
      " )                                                                          \n",
      "                                                                            \n",
      " flatten (Flatten)           multiple                  0         Y          \n",
      "                                                                            \n",
      " dense (Dense)               multiple                  4014336   Y          \n",
      "                                                                            \n",
      " dense_1 (Dense)             multiple                  2570      Y          \n",
      "                                                                            \n",
      "============================================================================\n",
      "Total params: 4,017,526\n",
      "Trainable params: 4,017,446\n",
      "Non-trainable params: 80\n",
      "____________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary(print_fn=None,\n",
    "    expand_nested=True,\n",
    "    show_trainable=True,\n",
    "    layer_range=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2f8dc4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.layers import GATConv, GCNConv, GeneralConv, GlobalSumPool, GlobalAvgPool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e388ddf",
   "metadata": {},
   "source": [
    "In the following we substitute GeneralConv with  GCNConv. This implies that also flatten has to be abbandoned in favour of GlobalSumPool or GlobalAvgPool, or in general a pooling layer. Still not clear though why this happens, I will pose the question on stackoverflow or will ask in some discord channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "310e7834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lavoro\\anaconda3\\envs\\GPU\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MNIST' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'inputs:0' shape=(50, 784, 1) dtype=float64>, <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x00000168D25817C0>) Tensor(\"target:0\", shape=(50,), dtype=uint8)\n",
      "(<tf.Tensor 'inputs:0' shape=(50, 784, 1) dtype=float64>, <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x00000168D1C13490>) Tensor(\"target:0\", shape=(50,), dtype=uint8)\n",
      "Train loss: 1.8824, acc: 0.2907 | Valid loss: 1.7656, acc: 0.3269 | Test loss: 1.7564, acc: 0.3290 | Elapsed time: 14.05s\n",
      "Train loss: 1.7414, acc: 0.3539 | Valid loss: 1.6225, acc: 0.3813 | Test loss: 1.6232, acc: 0.3806 | Elapsed time: 28.19s\n",
      "Train loss: 1.6071, acc: 0.4077 | Valid loss: 1.5323, acc: 0.4304 | Test loss: 1.5170, acc: 0.4379 | Elapsed time: 40.61s\n",
      "Train loss: 1.5463, acc: 0.4317 | Valid loss: 1.4696, acc: 0.4662 | Test loss: 1.4613, acc: 0.4622 | Elapsed time: 53.96s\n",
      "Train loss: 1.5185, acc: 0.4433 | Valid loss: 1.4430, acc: 0.4748 | Test loss: 1.4467, acc: 0.4693 | Elapsed time: 67.02s\n",
      "Train loss: 1.5059, acc: 0.4504 | Valid loss: 1.4837, acc: 0.4268 | Test loss: 1.4467, acc: 0.4693 | Elapsed time: 76.14s\n",
      "Train loss: 1.4794, acc: 0.4624 | Valid loss: 1.4229, acc: 0.4778 | Test loss: 1.4164, acc: 0.4786 | Elapsed time: 89.30s\n",
      "Train loss: 1.4496, acc: 0.4694 | Valid loss: 1.3758, acc: 0.4907 | Test loss: 1.3891, acc: 0.4770 | Elapsed time: 102.51s\n",
      "Train loss: 1.4360, acc: 0.4776 | Valid loss: 1.4154, acc: 0.4659 | Test loss: 1.3891, acc: 0.4770 | Elapsed time: 112.71s\n",
      "Train loss: 1.4216, acc: 0.4803 | Valid loss: 1.3402, acc: 0.5097 | Test loss: 1.3425, acc: 0.4991 | Elapsed time: 125.97s\n",
      "Train loss: 1.4135, acc: 0.4858 | Valid loss: 1.4001, acc: 0.4715 | Test loss: 1.3425, acc: 0.4991 | Elapsed time: 135.93s\n",
      "Train loss: 1.4055, acc: 0.4875 | Valid loss: 1.3452, acc: 0.4912 | Test loss: 1.3425, acc: 0.4991 | Elapsed time: 145.77s\n",
      "Train loss: 1.4006, acc: 0.4888 | Valid loss: 1.3432, acc: 0.4985 | Test loss: 1.3425, acc: 0.4991 | Elapsed time: 156.12s\n",
      "Train loss: 1.3877, acc: 0.4936 | Valid loss: 1.3331, acc: 0.5200 | Test loss: 1.3337, acc: 0.5165 | Elapsed time: 169.70s\n",
      "Train loss: 1.3826, acc: 0.4947 | Valid loss: 1.3654, acc: 0.4998 | Test loss: 1.3337, acc: 0.5165 | Elapsed time: 179.96s\n",
      "Train loss: 1.3688, acc: 0.5020 | Valid loss: 1.3336, acc: 0.5122 | Test loss: 1.3337, acc: 0.5165 | Elapsed time: 188.99s\n",
      "Train loss: 1.3675, acc: 0.4999 | Valid loss: 1.3121, acc: 0.5205 | Test loss: 1.3096, acc: 0.5200 | Elapsed time: 201.55s\n",
      "Train loss: 1.3660, acc: 0.5013 | Valid loss: 1.3492, acc: 0.5067 | Test loss: 1.3096, acc: 0.5200 | Elapsed time: 211.08s\n",
      "Train loss: 1.3713, acc: 0.4990 | Valid loss: 1.3061, acc: 0.5184 | Test loss: 1.3195, acc: 0.5086 | Elapsed time: 224.81s\n",
      "Train loss: 1.3613, acc: 0.5028 | Valid loss: 1.3327, acc: 0.5083 | Test loss: 1.3195, acc: 0.5086 | Elapsed time: 234.53s\n",
      "Train loss: 1.3580, acc: 0.5046 | Valid loss: 1.2868, acc: 0.5308 | Test loss: 1.2884, acc: 0.5283 | Elapsed time: 248.74s\n",
      "Train loss: 1.3548, acc: 0.5051 | Valid loss: 1.3616, acc: 0.4891 | Test loss: 1.2884, acc: 0.5283 | Elapsed time: 258.81s\n",
      "Train loss: 1.3548, acc: 0.5061 | Valid loss: 1.2641, acc: 0.5343 | Test loss: 1.2776, acc: 0.5199 | Elapsed time: 273.92s\n",
      "Train loss: 1.3511, acc: 0.5062 | Valid loss: 1.2661, acc: 0.5316 | Test loss: 1.2776, acc: 0.5199 | Elapsed time: 284.20s\n",
      "Train loss: 1.3525, acc: 0.5069 | Valid loss: 1.3506, acc: 0.5014 | Test loss: 1.2776, acc: 0.5199 | Elapsed time: 294.83s\n",
      "Train loss: 1.3413, acc: 0.5086 | Valid loss: 1.2681, acc: 0.5346 | Test loss: 1.2776, acc: 0.5199 | Elapsed time: 316.74s\n",
      "Train loss: 1.3382, acc: 0.5133 | Valid loss: 1.2683, acc: 0.5309 | Test loss: 1.2776, acc: 0.5199 | Elapsed time: 327.55s\n",
      "Train loss: 1.3461, acc: 0.5075 | Valid loss: 1.3202, acc: 0.5170 | Test loss: 1.2776, acc: 0.5199 | Elapsed time: 337.49s\n",
      "Train loss: 1.3316, acc: 0.5146 | Valid loss: 1.2572, acc: 0.5409 | Test loss: 1.2731, acc: 0.5242 | Elapsed time: 354.81s\n",
      "Train loss: 1.3326, acc: 0.5130 | Valid loss: 1.2651, acc: 0.5315 | Test loss: 1.2731, acc: 0.5242 | Elapsed time: 365.45s\n",
      "Train loss: 1.3205, acc: 0.5161 | Valid loss: 1.2452, acc: 0.5404 | Test loss: 1.2595, acc: 0.5379 | Elapsed time: 379.66s\n",
      "Train loss: 1.3267, acc: 0.5160 | Valid loss: 1.3561, acc: 0.5123 | Test loss: 1.2595, acc: 0.5379 | Elapsed time: 388.25s\n",
      "Train loss: 1.3148, acc: 0.5218 | Valid loss: 1.2653, acc: 0.5429 | Test loss: 1.2595, acc: 0.5379 | Elapsed time: 396.76s\n",
      "Train loss: 1.3295, acc: 0.5156 | Valid loss: 1.2619, acc: 0.5363 | Test loss: 1.2595, acc: 0.5379 | Elapsed time: 405.71s\n",
      "Train loss: 1.3152, acc: 0.5211 | Valid loss: 1.3335, acc: 0.5055 | Test loss: 1.2595, acc: 0.5379 | Elapsed time: 414.63s\n",
      "Train loss: 1.3189, acc: 0.5179 | Valid loss: 1.3756, acc: 0.4873 | Test loss: 1.2595, acc: 0.5379 | Elapsed time: 423.18s\n",
      "Train loss: 1.3155, acc: 0.5190 | Valid loss: 1.2654, acc: 0.5282 | Test loss: 1.2595, acc: 0.5379 | Elapsed time: 431.62s\n",
      "Train loss: 1.3041, acc: 0.5262 | Valid loss: 1.2401, acc: 0.5486 | Test loss: 1.2582, acc: 0.5419 | Elapsed time: 443.80s\n",
      "Train loss: 1.3036, acc: 0.5242 | Valid loss: 1.2861, acc: 0.5198 | Test loss: 1.2582, acc: 0.5419 | Elapsed time: 451.77s\n",
      "Train loss: 1.3046, acc: 0.5260 | Valid loss: 1.2398, acc: 0.5494 | Test loss: 1.2623, acc: 0.5365 | Elapsed time: 462.96s\n",
      "Train loss: 1.3064, acc: 0.5249 | Valid loss: 1.2947, acc: 0.5245 | Test loss: 1.2623, acc: 0.5365 | Elapsed time: 471.23s\n",
      "Train loss: 1.2969, acc: 0.5263 | Valid loss: 1.3457, acc: 0.5091 | Test loss: 1.2623, acc: 0.5365 | Elapsed time: 479.56s\n",
      "Train loss: 1.2958, acc: 0.5293 | Valid loss: 1.2325, acc: 0.5509 | Test loss: 1.2576, acc: 0.5384 | Elapsed time: 491.49s\n",
      "Train loss: 1.2958, acc: 0.5286 | Valid loss: 1.2985, acc: 0.5162 | Test loss: 1.2576, acc: 0.5384 | Elapsed time: 500.17s\n",
      "Train loss: 1.2889, acc: 0.5310 | Valid loss: 1.2018, acc: 0.5621 | Test loss: 1.2305, acc: 0.5473 | Elapsed time: 512.07s\n",
      "Train loss: 1.2971, acc: 0.5292 | Valid loss: 1.2376, acc: 0.5474 | Test loss: 1.2305, acc: 0.5473 | Elapsed time: 520.71s\n",
      "Train loss: 1.2814, acc: 0.5361 | Valid loss: 1.2322, acc: 0.5456 | Test loss: 1.2305, acc: 0.5473 | Elapsed time: 528.56s\n",
      "Train loss: 1.2866, acc: 0.5310 | Valid loss: 1.2132, acc: 0.5546 | Test loss: 1.2305, acc: 0.5473 | Elapsed time: 536.90s\n",
      "Train loss: 1.2826, acc: 0.5348 | Valid loss: 1.2299, acc: 0.5401 | Test loss: 1.2305, acc: 0.5473 | Elapsed time: 545.38s\n",
      "Train loss: 1.2847, acc: 0.5326 | Valid loss: 1.2077, acc: 0.5521 | Test loss: 1.2305, acc: 0.5473 | Elapsed time: 554.03s\n",
      "Train loss: 1.2766, acc: 0.5367 | Valid loss: 1.2386, acc: 0.5424 | Test loss: 1.2305, acc: 0.5473 | Elapsed time: 562.38s\n",
      "Train loss: 1.2770, acc: 0.5356 | Valid loss: 1.1857, acc: 0.5685 | Test loss: 1.2056, acc: 0.5565 | Elapsed time: 574.68s\n",
      "Train loss: 1.2747, acc: 0.5377 | Valid loss: 1.1949, acc: 0.5624 | Test loss: 1.2056, acc: 0.5565 | Elapsed time: 583.37s\n",
      "Train loss: 1.2663, acc: 0.5405 | Valid loss: 1.2656, acc: 0.5294 | Test loss: 1.2056, acc: 0.5565 | Elapsed time: 592.32s\n",
      "Train loss: 1.2716, acc: 0.5403 | Valid loss: 1.2563, acc: 0.5450 | Test loss: 1.2056, acc: 0.5565 | Elapsed time: 601.10s\n",
      "Train loss: 1.2662, acc: 0.5404 | Valid loss: 1.2124, acc: 0.5619 | Test loss: 1.2056, acc: 0.5565 | Elapsed time: 609.91s\n",
      "Train loss: 1.2676, acc: 0.5422 | Valid loss: 1.2093, acc: 0.5525 | Test loss: 1.2056, acc: 0.5565 | Elapsed time: 618.42s\n",
      "Train loss: 1.2639, acc: 0.5424 | Valid loss: 1.2179, acc: 0.5588 | Test loss: 1.2056, acc: 0.5565 | Elapsed time: 626.94s\n",
      "Train loss: 1.2638, acc: 0.5428 | Valid loss: 1.1897, acc: 0.5669 | Test loss: 1.2056, acc: 0.5565 | Elapsed time: 635.61s\n",
      "Train loss: 1.2611, acc: 0.5412 | Valid loss: 1.1935, acc: 0.5664 | Test loss: 1.2056, acc: 0.5565 | Elapsed time: 644.40s\n",
      "Train loss: 1.2559, acc: 0.5435 | Valid loss: 1.1889, acc: 0.5662 | Test loss: 1.2056, acc: 0.5565 | Elapsed time: 653.39s\n",
      "Train loss: 1.2624, acc: 0.5422 | Valid loss: 1.1682, acc: 0.5758 | Test loss: 1.2014, acc: 0.5641 | Elapsed time: 666.00s\n",
      "Train loss: 1.2539, acc: 0.5464 | Valid loss: 1.1785, acc: 0.5618 | Test loss: 1.2014, acc: 0.5641 | Elapsed time: 674.77s\n",
      "Train loss: 1.2508, acc: 0.5467 | Valid loss: 1.2548, acc: 0.5384 | Test loss: 1.2014, acc: 0.5641 | Elapsed time: 683.11s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.2490, acc: 0.5491 | Valid loss: 1.2094, acc: 0.5564 | Test loss: 1.2014, acc: 0.5641 | Elapsed time: 691.53s\n",
      "Train loss: 1.2487, acc: 0.5478 | Valid loss: 1.1827, acc: 0.5705 | Test loss: 1.2014, acc: 0.5641 | Elapsed time: 699.82s\n",
      "Train loss: 1.2461, acc: 0.5490 | Valid loss: 1.2128, acc: 0.5449 | Test loss: 1.2014, acc: 0.5641 | Elapsed time: 708.15s\n",
      "Train loss: 1.2533, acc: 0.5483 | Valid loss: 1.1908, acc: 0.5664 | Test loss: 1.2014, acc: 0.5641 | Elapsed time: 716.86s\n",
      "Train loss: 1.2420, acc: 0.5510 | Valid loss: 1.1509, acc: 0.5769 | Test loss: 1.1738, acc: 0.5696 | Elapsed time: 729.34s\n",
      "Train loss: 1.2448, acc: 0.5507 | Valid loss: 1.1750, acc: 0.5685 | Test loss: 1.1738, acc: 0.5696 | Elapsed time: 737.96s\n",
      "Train loss: 1.2435, acc: 0.5533 | Valid loss: 1.1585, acc: 0.5731 | Test loss: 1.1738, acc: 0.5696 | Elapsed time: 746.89s\n",
      "Train loss: 1.2350, acc: 0.5552 | Valid loss: 1.2056, acc: 0.5607 | Test loss: 1.1738, acc: 0.5696 | Elapsed time: 755.66s\n",
      "Train loss: 1.2286, acc: 0.5542 | Valid loss: 1.2022, acc: 0.5647 | Test loss: 1.1738, acc: 0.5696 | Elapsed time: 764.05s\n",
      "Train loss: 1.2377, acc: 0.5519 | Valid loss: 1.1636, acc: 0.5705 | Test loss: 1.1738, acc: 0.5696 | Elapsed time: 772.41s\n",
      "Train loss: 1.2310, acc: 0.5550 | Valid loss: 1.1562, acc: 0.5788 | Test loss: 1.1738, acc: 0.5696 | Elapsed time: 781.23s\n",
      "Train loss: 1.2278, acc: 0.5575 | Valid loss: 1.2688, acc: 0.5250 | Test loss: 1.1738, acc: 0.5696 | Elapsed time: 789.84s\n",
      "Train loss: 1.2321, acc: 0.5559 | Valid loss: 1.1568, acc: 0.5794 | Test loss: 1.1738, acc: 0.5696 | Elapsed time: 798.39s\n",
      "Train loss: 1.2222, acc: 0.5615 | Valid loss: 1.1950, acc: 0.5635 | Test loss: 1.1738, acc: 0.5696 | Elapsed time: 806.31s\n",
      "Train loss: 1.2274, acc: 0.5562 | Valid loss: 1.1440, acc: 0.5877 | Test loss: 1.1730, acc: 0.5737 | Elapsed time: 817.77s\n",
      "Train loss: 1.2199, acc: 0.5594 | Valid loss: 1.1364, acc: 0.5971 | Test loss: 1.1656, acc: 0.5780 | Elapsed time: 829.35s\n",
      "Train loss: 1.2189, acc: 0.5628 | Valid loss: 1.2062, acc: 0.5577 | Test loss: 1.1656, acc: 0.5780 | Elapsed time: 837.55s\n",
      "Train loss: 1.2177, acc: 0.5615 | Valid loss: 1.1437, acc: 0.5925 | Test loss: 1.1656, acc: 0.5780 | Elapsed time: 845.57s\n",
      "Train loss: 1.2155, acc: 0.5610 | Valid loss: 1.2787, acc: 0.5293 | Test loss: 1.1656, acc: 0.5780 | Elapsed time: 853.83s\n",
      "Train loss: 1.2216, acc: 0.5600 | Valid loss: 1.1230, acc: 0.5902 | Test loss: 1.1535, acc: 0.5759 | Elapsed time: 865.85s\n",
      "Train loss: 1.2153, acc: 0.5611 | Valid loss: 1.1555, acc: 0.5844 | Test loss: 1.1535, acc: 0.5759 | Elapsed time: 873.97s\n",
      "Train loss: 1.2158, acc: 0.5611 | Valid loss: 1.1301, acc: 0.6013 | Test loss: 1.1535, acc: 0.5759 | Elapsed time: 882.18s\n",
      "Train loss: 1.2089, acc: 0.5643 | Valid loss: 1.1305, acc: 0.5892 | Test loss: 1.1535, acc: 0.5759 | Elapsed time: 890.54s\n",
      "Train loss: 1.2128, acc: 0.5628 | Valid loss: 1.2300, acc: 0.5518 | Test loss: 1.1535, acc: 0.5759 | Elapsed time: 898.73s\n",
      "Train loss: 1.2052, acc: 0.5657 | Valid loss: 1.1075, acc: 0.6029 | Test loss: 1.1376, acc: 0.5886 | Elapsed time: 910.86s\n",
      "Train loss: 1.2046, acc: 0.5674 | Valid loss: 1.2553, acc: 0.5414 | Test loss: 1.1376, acc: 0.5886 | Elapsed time: 919.06s\n",
      "Train loss: 1.1946, acc: 0.5701 | Valid loss: 1.1011, acc: 0.6021 | Test loss: 1.1340, acc: 0.5885 | Elapsed time: 930.69s\n",
      "Train loss: 1.2009, acc: 0.5685 | Valid loss: 1.1222, acc: 0.6028 | Test loss: 1.1340, acc: 0.5885 | Elapsed time: 939.27s\n",
      "Train loss: 1.2035, acc: 0.5683 | Valid loss: 1.1897, acc: 0.5732 | Test loss: 1.1340, acc: 0.5885 | Elapsed time: 947.47s\n",
      "Train loss: 1.1950, acc: 0.5711 | Valid loss: 1.1721, acc: 0.5737 | Test loss: 1.1340, acc: 0.5885 | Elapsed time: 955.57s\n",
      "Train loss: 1.1892, acc: 0.5730 | Valid loss: 1.1505, acc: 0.5776 | Test loss: 1.1340, acc: 0.5885 | Elapsed time: 963.90s\n",
      "Train loss: 1.1987, acc: 0.5688 | Valid loss: 1.1259, acc: 0.5875 | Test loss: 1.1340, acc: 0.5885 | Elapsed time: 972.15s\n",
      "Train loss: 1.1887, acc: 0.5740 | Valid loss: 1.1539, acc: 0.5825 | Test loss: 1.1340, acc: 0.5885 | Elapsed time: 980.13s\n",
      "Train loss: 1.1904, acc: 0.5711 | Valid loss: 1.0998, acc: 0.6095 | Test loss: 1.1337, acc: 0.5891 | Elapsed time: 992.43s\n",
      "Train loss: 1.1885, acc: 0.5730 | Valid loss: 1.1284, acc: 0.5838 | Test loss: 1.1337, acc: 0.5891 | Elapsed time: 1001.67s\n",
      "Train loss: 1.1834, acc: 0.5762 | Valid loss: 1.1187, acc: 0.6022 | Test loss: 1.1337, acc: 0.5891 | Elapsed time: 1010.64s\n",
      "Train loss: 1.1752, acc: 0.5799 | Valid loss: 1.1611, acc: 0.5811 | Test loss: 1.1337, acc: 0.5891 | Elapsed time: 1018.73s\n",
      "Train loss: 1.1900, acc: 0.5724 | Valid loss: 1.0933, acc: 0.6117 | Test loss: 1.1321, acc: 0.5906 | Elapsed time: 1030.09s\n",
      "Train loss: 1.1841, acc: 0.5745 | Valid loss: 1.1849, acc: 0.5735 | Test loss: 1.1321, acc: 0.5906 | Elapsed time: 1038.30s\n",
      "Train loss: 1.1789, acc: 0.5770 | Valid loss: 1.1019, acc: 0.6057 | Test loss: 1.1321, acc: 0.5906 | Elapsed time: 1046.44s\n",
      "Train loss: 1.1870, acc: 0.5739 | Valid loss: 1.1071, acc: 0.5952 | Test loss: 1.1321, acc: 0.5906 | Elapsed time: 1054.50s\n",
      "Train loss: 1.1814, acc: 0.5756 | Valid loss: 1.1666, acc: 0.5837 | Test loss: 1.1321, acc: 0.5906 | Elapsed time: 1063.40s\n",
      "Train loss: 1.1775, acc: 0.5770 | Valid loss: 1.0883, acc: 0.6118 | Test loss: 1.1237, acc: 0.5931 | Elapsed time: 1075.96s\n",
      "Train loss: 1.1728, acc: 0.5794 | Valid loss: 1.1026, acc: 0.5943 | Test loss: 1.1237, acc: 0.5931 | Elapsed time: 1084.38s\n",
      "Train loss: 1.1765, acc: 0.5766 | Valid loss: 1.0844, acc: 0.6133 | Test loss: 1.1195, acc: 0.5938 | Elapsed time: 1097.36s\n",
      "Train loss: 1.1691, acc: 0.5817 | Valid loss: 1.1018, acc: 0.6088 | Test loss: 1.1195, acc: 0.5938 | Elapsed time: 1106.20s\n",
      "Train loss: 1.1791, acc: 0.5745 | Valid loss: 1.1445, acc: 0.5837 | Test loss: 1.1195, acc: 0.5938 | Elapsed time: 1115.25s\n",
      "Train loss: 1.1662, acc: 0.5836 | Valid loss: 1.0962, acc: 0.6068 | Test loss: 1.1195, acc: 0.5938 | Elapsed time: 1123.85s\n",
      "Train loss: 1.1692, acc: 0.5811 | Valid loss: 1.0830, acc: 0.6111 | Test loss: 1.1176, acc: 0.5956 | Elapsed time: 1136.86s\n",
      "Train loss: 1.1682, acc: 0.5815 | Valid loss: 1.0920, acc: 0.6166 | Test loss: 1.1176, acc: 0.5956 | Elapsed time: 1145.64s\n",
      "Train loss: 1.1524, acc: 0.5883 | Valid loss: 1.1014, acc: 0.5910 | Test loss: 1.1176, acc: 0.5956 | Elapsed time: 1154.31s\n",
      "Train loss: 1.1660, acc: 0.5812 | Valid loss: 1.1344, acc: 0.5935 | Test loss: 1.1176, acc: 0.5956 | Elapsed time: 1162.99s\n",
      "Train loss: 1.1551, acc: 0.5877 | Valid loss: 1.1292, acc: 0.5966 | Test loss: 1.1176, acc: 0.5956 | Elapsed time: 1171.98s\n",
      "Train loss: 1.1521, acc: 0.5858 | Valid loss: 1.0980, acc: 0.5951 | Test loss: 1.1176, acc: 0.5956 | Elapsed time: 1180.35s\n",
      "Train loss: 1.1715, acc: 0.5795 | Valid loss: 1.0947, acc: 0.5989 | Test loss: 1.1176, acc: 0.5956 | Elapsed time: 1188.78s\n",
      "Train loss: 1.1581, acc: 0.5862 | Valid loss: 1.1147, acc: 0.5945 | Test loss: 1.1176, acc: 0.5956 | Elapsed time: 1197.43s\n",
      "Train loss: 1.1541, acc: 0.5867 | Valid loss: 1.2104, acc: 0.5559 | Test loss: 1.1176, acc: 0.5956 | Elapsed time: 1205.54s\n",
      "Train loss: 1.1559, acc: 0.5864 | Valid loss: 1.2077, acc: 0.5603 | Test loss: 1.1176, acc: 0.5956 | Elapsed time: 1213.57s\n",
      "Train loss: 1.1497, acc: 0.5891 | Valid loss: 1.0619, acc: 0.6163 | Test loss: 1.1012, acc: 0.5961 | Elapsed time: 1224.75s\n",
      "Train loss: 1.1500, acc: 0.5896 | Valid loss: 1.0867, acc: 0.6071 | Test loss: 1.1012, acc: 0.5961 | Elapsed time: 1232.72s\n",
      "Train loss: 1.1411, acc: 0.5931 | Valid loss: 1.2194, acc: 0.5567 | Test loss: 1.1012, acc: 0.5961 | Elapsed time: 1240.60s\n",
      "Train loss: 1.1408, acc: 0.5931 | Valid loss: 1.0824, acc: 0.6056 | Test loss: 1.1012, acc: 0.5961 | Elapsed time: 1248.53s\n",
      "Train loss: 1.1494, acc: 0.5879 | Valid loss: 1.1255, acc: 0.5941 | Test loss: 1.1012, acc: 0.5961 | Elapsed time: 1257.06s\n",
      "Train loss: 1.1550, acc: 0.5859 | Valid loss: 1.0785, acc: 0.6106 | Test loss: 1.1012, acc: 0.5961 | Elapsed time: 1265.76s\n",
      "Train loss: 1.1453, acc: 0.5905 | Valid loss: 1.1108, acc: 0.5974 | Test loss: 1.1012, acc: 0.5961 | Elapsed time: 1274.14s\n",
      "Train loss: 1.1479, acc: 0.5929 | Valid loss: 1.0950, acc: 0.5973 | Test loss: 1.1012, acc: 0.5961 | Elapsed time: 1283.09s\n",
      "Train loss: 1.1449, acc: 0.5896 | Valid loss: 1.0701, acc: 0.6137 | Test loss: 1.1012, acc: 0.5961 | Elapsed time: 1291.80s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1436, acc: 0.5912 | Valid loss: 1.3294, acc: 0.5128 | Test loss: 1.1012, acc: 0.5961 | Elapsed time: 1300.03s\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "data = MNIST()\n",
    "adj=data.a\n",
    "\n",
    "# Create filter for GCN and convert to sparse tensor.\n",
    "data.a = GeneralConv.preprocess(data.a)\n",
    "data.a = sp_matrix_to_sp_tensor(data.a)\n",
    "\n",
    "# Train/valid/test split\n",
    "data_tr, data_te = data[:-10000], data[-10000:]\n",
    "data_tr, data_va = data_tr[:-10000], data_tr[-10000:]\n",
    "\n",
    "# We use a MixedLoader since the dataset is in mixed mode\n",
    "loader_tr = MixedLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = MixedLoader(data_va, batch_size=batch_size)\n",
    "loader_te = MixedLoader(data_te, batch_size=batch_size)\n",
    "\n",
    "# Specify device to be used by TensorFlow\n",
    "device = '/device:GPU:0'\n",
    "\n",
    "# Build model\n",
    "class Net(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv1 = GCNConv(32, activation=\"elu\", kernel_regularizer=l2(l2_reg), use_bias=True)\n",
    "        self.conv2 = GCNConv(32, activation=\"elu\", kernel_regularizer=l2(l2_reg), use_bias=True)\n",
    "        self.conv3 = GCNConv(32, activation=\"elu\", kernel_regularizer=l2(l2_reg), use_bias=True)\n",
    "        self.flatten = GlobalAvgPool()\n",
    "        self.fc1 = Dense(512, activation=\"relu\")\n",
    "        self.fc2 = Dense(10, activation=\"softmax\")  # MNIST has 10 classes\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        with tf.device(device):\n",
    "            x = self.conv1([x, a])\n",
    "            x = self.conv2([x, a])\n",
    "            output = self.flatten(x)\n",
    "            output = self.fc1(output)\n",
    "            output = self.fc2(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Create model\n",
    "with tf.device(device):\n",
    "    model = Net()\n",
    "    optimizer = Adam()\n",
    "    loss_fn = SparseCategoricalCrossentropy()\n",
    "    \n",
    "\n",
    "# Training function\n",
    "@tf.function\n",
    "def train_on_batch(inputs, target):\n",
    "    with tf.device(device):\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            predictions = model(inputs, training=True)\n",
    "            loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "            acc = tf.reduce_mean(sparse_categorical_accuracy(target, predictions))\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss, acc\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(loader):\n",
    "    step = 0\n",
    "    results = []\n",
    "    for batch in loader:\n",
    "        step += 1\n",
    "        inputs,target = batch\n",
    "        \n",
    "        with tf.device(device):\n",
    "            predictions = model(inputs, training=False)\n",
    "            loss = loss_fn(target, predictions)\n",
    "            acc = tf.reduce_mean(sparse_categorical_accuracy(target, predictions))\n",
    "            results.append((loss, acc, len(target)))  # Keep track of batch size\n",
    "            if step == loader.steps_per_epoch:\n",
    "                results = np.array(results)\n",
    "                return np.average(results[:, :-1], 0, weights=results[:, -1])\n",
    "            \n",
    "\n",
    "\n",
    "# Setup training\n",
    "best_val_loss = 10000\n",
    "current_patience = patience\n",
    "\n",
    "\n",
    "# Training loop\n",
    "results_tr = []\n",
    "step=0\n",
    "import time\n",
    "start_time=time.time()\n",
    "for batch in loader_tr:\n",
    "    step += 1\n",
    "  \n",
    "    # Training step\n",
    "    inputs, target = batch\n",
    "\n",
    "    loss, acc = train_on_batch(inputs, target)\n",
    "    results_tr.append((loss, acc, len(target)))\n",
    "    \n",
    "    if step == loader_tr.steps_per_epoch:\n",
    "        results_va = evaluate(loader_va)\n",
    "        if results_va[0] < best_val_loss:\n",
    "            best_val_loss = results_va[0]\n",
    "            current_patience = patience\n",
    "            results_te = evaluate(loader_te)\n",
    "        else:\n",
    "            current_patience -= 1\n",
    "            if current_patience == 0:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        # Print results\n",
    "        results_tr = np.array(results_tr)\n",
    "        results_tr = np.average(results_tr[:, :-1], 0, weights=results_tr[:, -1])\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        print(\n",
    "            \"Train loss: {:.4f}, acc: {:.4f} | \"\n",
    "            \"Valid loss: {:.4f}, acc: {:.4f} | \"\n",
    "            \"Test loss: {:.4f}, acc: {:.4f} | \"\n",
    "            \"Elapsed time: {:.2f}s\".format(\n",
    "                *results_tr, *results_va, *results_te, elapsed_time\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Reset epoch\n",
    "        results_tr = []\n",
    "        step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e861ff14",
   "metadata": {},
   "source": [
    "Comments: The accuracy here stabilized close to 60%. This is most likely a problem in network expressivity, the absence of the flatten layer reduces drastically the number of parameters involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf7a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary(print_fn=None,\n",
    "    expand_nested=True,\n",
    "    show_trainable=True,\n",
    "    layer_range=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5283b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPUforNN",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
